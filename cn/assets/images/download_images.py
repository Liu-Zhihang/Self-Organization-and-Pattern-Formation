#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å›¾ç‰‡ä¸‹è½½è„šæœ¬ - è‡ªç»„ç»‡ä¸æ¨¡å¼å½¢æˆè¯¾ç¨‹ç¬”è®°

åŠŸèƒ½ï¼š
1. æ‰«æ docs/cn/ ç›®å½•ä¸‹æ‰€æœ‰ Markdown æ–‡ä»¶
2. æå–æ–‡ä»¶ä¸­çš„å›¾ç‰‡ URLï¼ˆæ”¯æŒ mdniceã€imgur ç­‰å¸¸è§å›¾åºŠï¼‰
3. ä¸‹è½½å›¾ç‰‡åˆ°æœ¬åœ° assets/images/ ç›®å½•
4. æ”¯æŒå¢é‡ä¸‹è½½ï¼ˆè·³è¿‡å·²ä¸‹è½½çš„å›¾ç‰‡ï¼‰
5. ç”Ÿæˆä¸‹è½½æ—¥å¿—ï¼Œè®°å½•å·²å¤„ç†çš„æ–‡ä»¶

ä½¿ç”¨æ–¹æ³•ï¼š
    python download_images.py

ä½œè€…ï¼šZhihang Liu
æ—¥æœŸï¼š2026-02
"""

import os
import re
import sys
import json
import hashlib
import requests
from pathlib import Path
from datetime import datetime
from urllib.parse import urlparse, unquote
from typing import Dict, List, Set, Tuple

# å¼ºåˆ¶åˆ·æ–°è¾“å‡º
def plog(msg: str):
    print(msg, flush=True)

# ==================== é…ç½® ====================

# è„šæœ¬æ‰€åœ¨ç›®å½•
SCRIPT_DIR = Path(__file__).parent.absolute()

# Markdown æ–‡ä»¶ç›®å½•ï¼ˆdocs/cn/assets/images -> docs/cnï¼‰
DOCS_DIR = SCRIPT_DIR.parent.parent

# å›¾ç‰‡ä¿å­˜ç›®å½•
IMAGES_DIR = SCRIPT_DIR

# ä¸‹è½½æ—¥å¿—æ–‡ä»¶ï¼ˆè®°å½•å·²å¤„ç†çš„æ–‡ä»¶å’Œå›¾ç‰‡ï¼‰
LOG_FILE = SCRIPT_DIR / "download_log.json"

# æ”¯æŒçš„å›¾ç‰‡æ ¼å¼
IMAGE_EXTENSIONS = {'.jpg', '.jpeg', '.png', '.gif', '.webp', '.svg', '.bmp'}

# è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
REQUEST_TIMEOUT = 15

# è¯·æ±‚å¤´ï¼ˆæ¨¡æ‹Ÿæµè§ˆå™¨ï¼‰
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'image/avif,image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8',
    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
}

# ==================== å·¥å…·å‡½æ•° ====================

def load_log() -> Dict:
    """åŠ è½½ä¸‹è½½æ—¥å¿—"""
    if LOG_FILE.exists():
        try:
            with open(LOG_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except (json.JSONDecodeError, IOError):
            pass
    return {
        "last_update": None,
        "processed_files": {},  # {æ–‡ä»¶å: {æœ€åä¿®æ”¹æ—¶é—´, å›¾ç‰‡æ•°é‡}}
        "downloaded_urls": {}   # {url: æœ¬åœ°æ–‡ä»¶å}
    }

def save_log(log: Dict) -> None:
    """ä¿å­˜ä¸‹è½½æ—¥å¿—"""
    log["last_update"] = datetime.now().isoformat()
    with open(LOG_FILE, 'w', encoding='utf-8') as f:
        json.dump(log, f, ensure_ascii=False, indent=2)

def get_file_hash(filepath: Path) -> str:
    """è·å–æ–‡ä»¶çš„ MD5 å“ˆå¸Œå€¼ï¼ˆç”¨äºæ£€æµ‹æ–‡ä»¶æ˜¯å¦ä¿®æ”¹ï¼‰"""
    hasher = hashlib.md5()
    with open(filepath, 'rb') as f:
        for chunk in iter(lambda: f.read(65536), b''):
            hasher.update(chunk)
    return hasher.hexdigest()

def extract_image_urls(content: str) -> List[str]:
    """
    ä» Markdown å†…å®¹ä¸­æå–å›¾ç‰‡ URL
    
    æ”¯æŒçš„æ ¼å¼ï¼š
    - ![alt](url)
    - ![alt](url "title")
    - <img src="url" ...>
    """
    urls = []
    
    # Markdown å›¾ç‰‡è¯­æ³•: ![alt](url) æˆ– ![alt](url "title")
    md_pattern = r'!\[([^\]]*)\]\(([^)\s]+)(?:\s+"[^"]*")?\)'
    for match in re.finditer(md_pattern, content):
        url = match.group(2)
        if url.startswith('http'):
            urls.append(url)
    
    # HTML img æ ‡ç­¾: <img src="url" ...>
    html_pattern = r'<img[^>]+src=["\']([^"\']+)["\']'
    for match in re.finditer(html_pattern, content, re.IGNORECASE):
        url = match.group(1)
        if url.startswith('http'):
            urls.append(url)
    
    return urls

def url_to_filename(url: str, index: int, note_name: str) -> str:
    """
    å°† URL è½¬æ¢ä¸ºæœ¬åœ°æ–‡ä»¶å
    
    æ ¼å¼ï¼š{ç¬”è®°ç¼–å·}_{åºå·}_{åŸå§‹æ–‡ä»¶åæˆ–å“ˆå¸Œ}
    ä¾‹å¦‚ï¼š01_001_853e3a67-9126-4927-907c-9f48eade8561.png
    """
    # è§£æ URL
    parsed = urlparse(url)
    path = unquote(parsed.path)
    
    # è·å–åŸå§‹æ–‡ä»¶å
    original_name = Path(path).name
    
    # è·å–æ‰©å±•å
    ext = Path(path).suffix.lower()
    if ext not in IMAGE_EXTENSIONS:
        ext = '.png'  # é»˜è®¤æ‰©å±•å
    
    # æå–ç¬”è®°ç¼–å·ï¼ˆä»æ–‡ä»¶åå¼€å¤´æå–æ•°å­—ï¼‰
    note_num_match = re.match(r'^(\d+)', note_name)
    note_num = note_num_match.group(1).zfill(2) if note_num_match else '00'
    
    # ç”Ÿæˆæ–‡ä»¶å
    # å¦‚æœåŸå§‹æ–‡ä»¶åæœ‰æ„ä¹‰ï¼ˆä¸æ˜¯çº¯å“ˆå¸Œï¼‰ï¼Œä¿ç•™ä¸€éƒ¨åˆ†
    if len(original_name) > 40:
        # å¯èƒ½æ˜¯ UUID æˆ–å“ˆå¸Œï¼Œæˆªå–ä¸€éƒ¨åˆ†
        name_part = Path(original_name).stem[:20]
    else:
        name_part = Path(original_name).stem
    
    # æ¸…ç†æ–‡ä»¶åä¸­çš„ç‰¹æ®Šå­—ç¬¦
    name_part = re.sub(r'[<>:"/\\|?*]', '_', name_part)
    
    return f"{note_num}_{str(index).zfill(3)}_{name_part}{ext}"

def download_image(url: str, save_path: Path) -> bool:
    """
    ä¸‹è½½å›¾ç‰‡åˆ°æŒ‡å®šè·¯å¾„
    
    è¿”å›ï¼šæ˜¯å¦æˆåŠŸ
    """
    try:
        response = requests.get(
            url, 
            headers=HEADERS, 
            timeout=REQUEST_TIMEOUT,
            stream=True
        )
        response.raise_for_status()
        
        # æ£€æŸ¥å†…å®¹ç±»å‹
        content_type = response.headers.get('Content-Type', '')
        if not content_type.startswith('image/'):
            print(f"  âš  è­¦å‘Šï¼šéå›¾ç‰‡å†…å®¹ç±»å‹ ({content_type})")
        
        # ä¿å­˜å›¾ç‰‡
        with open(save_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        
        return True
        
    except requests.exceptions.RequestException as e:
        print(f"  âœ— ä¸‹è½½å¤±è´¥ï¼š{e}")
        return False

# ==================== ä¸»é€»è¾‘ ====================

def process_markdown_file(md_file: Path, log: Dict) -> Tuple[int, int]:
    """
    å¤„ç†å•ä¸ª Markdown æ–‡ä»¶
    
    è¿”å›ï¼š(æ–°ä¸‹è½½æ•°é‡, è·³è¿‡æ•°é‡)
    """
    note_name = md_file.stem
    print(f"\nğŸ“„ å¤„ç†æ–‡ä»¶ï¼š{note_name}")
    
    # è¯»å–æ–‡ä»¶å†…å®¹
    with open(md_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # æå–å›¾ç‰‡ URL
    urls = extract_image_urls(content)
    
    if not urls:
        print("  (æ— å›¾ç‰‡)")
        return 0, 0
    
    print(f"  å‘ç° {len(urls)} å¼ å›¾ç‰‡")
    
    downloaded = 0
    skipped = 0
    
    for i, url in enumerate(urls, 1):
        # æ£€æŸ¥æ˜¯å¦å·²ä¸‹è½½
        if url in log["downloaded_urls"]:
            local_file = log["downloaded_urls"][url]
            if (IMAGES_DIR / local_file).exists():
                skipped += 1
                continue
        
        # ç”Ÿæˆæœ¬åœ°æ–‡ä»¶å
        filename = url_to_filename(url, i, note_name)
        save_path = IMAGES_DIR / filename
        
        # é¿å…æ–‡ä»¶åå†²çª
        counter = 1
        while save_path.exists() and url not in log["downloaded_urls"]:
            stem = Path(filename).stem
            ext = Path(filename).suffix
            filename = f"{stem}_{counter}{ext}"
            save_path = IMAGES_DIR / filename
            counter += 1
        
        # ä¸‹è½½å›¾ç‰‡
        print(f"  [{i}/{len(urls)}] ä¸‹è½½ï¼š{filename[:50]}...")
        if download_image(url, save_path):
            log["downloaded_urls"][url] = filename
            downloaded += 1
            print(f"  âœ“ æˆåŠŸ")
        else:
            print(f"  âœ— å¤±è´¥")
    
    return downloaded, skipped

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸ“¥ å›¾ç‰‡ä¸‹è½½è„šæœ¬ - è‡ªç»„ç»‡ä¸æ¨¡å¼å½¢æˆè¯¾ç¨‹ç¬”è®°")
    print("=" * 60)
    
    # ç¡®ä¿å›¾ç‰‡ç›®å½•å­˜åœ¨
    IMAGES_DIR.mkdir(parents=True, exist_ok=True)
    
    # åŠ è½½æ—¥å¿—
    log = load_log()
    
    # è·å–æ‰€æœ‰ Markdown æ–‡ä»¶
    md_files = sorted(DOCS_DIR.glob("*.md"))
    
    if not md_files:
        print(f"\nâš  æœªæ‰¾åˆ° Markdown æ–‡ä»¶ï¼š{DOCS_DIR}")
        return
    
    print(f"\næ‰¾åˆ° {len(md_files)} ä¸ª Markdown æ–‡ä»¶")
    
    # ç»Ÿè®¡
    total_downloaded = 0
    total_skipped = 0
    processed_count = 0
    
    for md_file in md_files:
        # è·å–æ–‡ä»¶ä¿®æ”¹æ—¶é—´
        mtime = md_file.stat().st_mtime
        file_key = md_file.name
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å¤„ç†ä¸”æœªä¿®æ”¹
        if file_key in log["processed_files"]:
            last_mtime = log["processed_files"][file_key].get("mtime", 0)
            if mtime <= last_mtime:
                print(f"\nâ­ è·³è¿‡æœªä¿®æ”¹æ–‡ä»¶ï¼š{md_file.stem}")
                continue
        
        # å¤„ç†æ–‡ä»¶
        downloaded, skipped = process_markdown_file(md_file, log)
        
        # æ›´æ–°æ—¥å¿—
        log["processed_files"][file_key] = {
            "mtime": mtime,
            "image_count": downloaded + skipped,
            "last_processed": datetime.now().isoformat()
        }
        
        total_downloaded += downloaded
        total_skipped += skipped
        processed_count += 1
    
    # ä¿å­˜æ—¥å¿—
    save_log(log)
    
    # è¾“å‡ºç»Ÿè®¡
    print("\n" + "=" * 60)
    print("ğŸ“Š ç»Ÿè®¡ä¿¡æ¯")
    print("=" * 60)
    print(f"  å¤„ç†æ–‡ä»¶æ•°ï¼š{processed_count}")
    print(f"  æ–°ä¸‹è½½å›¾ç‰‡ï¼š{total_downloaded}")
    print(f"  è·³è¿‡å·²æœ‰å›¾ç‰‡ï¼š{total_skipped}")
    print(f"  å›¾ç‰‡ä¿å­˜ç›®å½•ï¼š{IMAGES_DIR}")
    print(f"  æ—¥å¿—æ–‡ä»¶ï¼š{LOG_FILE}")
    print("=" * 60)

if __name__ == "__main__":
    main()
